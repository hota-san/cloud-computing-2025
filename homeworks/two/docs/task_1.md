# Hadoop: Distributed Data Processing for Initial Aggregation

**Objective:** To implement a basic MapReduce job and demonstrate its application for initial data aggregation within a Federated Learning workflow.

**Instructions:**

1.  **Sample Dataset Acquisition:**
    * A sample CSV dataset simulating client-specific data has been provided. This dataset is accessible here with the name of [data.csv](../data.csv)
     The dataset contains columns such as `client_id`, `feature_value`, and supplementary (non-essential) data.
    * Example CSV structure:

    ```csv
    client_id,feature_value,other_data
    client1,10.5,abc
    client2,15.2,def
    client1,12.0,ghi
    client3,20.0,jkl
    client2,16.8,mno
    client3,22.5,pqr
    ```

    * **Action:** Download this file and store it within your local development environment.

2.  **MapReduce Job Implementation:**
    * **Programming Language Selection:** You may employ either Java or Python, utilizing Hadoop Streaming for the implementation of the MapReduce job.
    * **Functional Requirements:** The MapReduce job must perform the following operations:
        * **Map Phase:**
            * Read the input CSV dataset.
            * Extract the `client_id` and `feature_value` columns.
            * Generate key-value pairs where the key is `client_id` and the value is formatted as `feature_value,1`. The "1" is a counter for subsequent aggregation.
        * **Reduce Phase:**
            * Group values based on the `client_id` key.
            * Calculate the sum of `feature_value` and the total count of occurrences for each `client_id`.
            * Output the `client_id`, the computed sum, and the total count.

3.  **Job Execution and Result Capture:**
    * Execute the developed MapReduce job on your Hadoop cluster or pseudo-distributed environment.
    * Capture screenshots documenting the job execution process (e.g., from the Hadoop web UI or terminal).
    * Record the output of the MapReduce job, which should contain the aggregated results for each `client_id`.

**Deliverables:**

1. **MapReduce Code:** Submit the Java or Python code file.
2. **Execution Screenshots:** Provide screenshots illustrating the Hadoop job execution.
3. **Job Output:** Submit the output generated by the Hadoop job.

**Evaluation Criteria:**

* Code will be assessed for correctness, clarity, and efficiency.
* Screenshots and output will be evaluated for completeness and accuracy.
* Code should be well-documented with clear comments (NOT BY GPT TOOLS).

**Guidance:**

* Begin with a simplified implementation and iteratively build upon it.
* Test the code on a small subset of the dataset before processing the entire dataset.
* Utilize the Hadoop web UI for job monitoring.
* Ensure proper formatting of the output.